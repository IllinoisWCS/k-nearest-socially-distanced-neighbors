{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "knn_memes_wcs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W83l6zvN4ctr"
      },
      "source": [
        "# WARNING: we cannot make any guarantees about the content of the memes. They were scraped automatically from r/wholesomememes. Proceed at your own discretion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Arzlt92aDKK9"
      },
      "source": [
        "### Let's get some data to work with!\n",
        "\n",
        "We'll be using the Reddit API (Python wrapper) to scrape data off of r/wholesomememes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE56cJS74Mfy"
      },
      "source": [
        "# importing packages that we need to run the program\n",
        "import psaw                     # reddit API to scrape posts from the internet\n",
        "import pandas as pd             # pandas is a dataframe library that lets us handle data cleanly in an organized manner\n",
        "from PIL import Image           # PIL is a python imaging library, we'll be using it to show images\n",
        "import requests                 # lets us makes HTTP requests to fetch the image from the URL\n",
        "from io import BytesIO          # lets us handle bytestreams, which is the return result of the HTTP request\n",
        "import numpy as np              # lets us handle array data very quickly through vectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e96UbUfT4wZ-"
      },
      "source": [
        "# scraping the data from reddit's r/wholesomememes\n",
        "# please don't run this cell more than once! you will be rate-limited\n",
        "\n",
        "# making an instance of the scraper\n",
        "scraper = psaw.PushshiftAPI()\n",
        "\n",
        "# querying reddit from r/wholesomememes and pulling info about the image url, gildings, number of comments, and number of crossposts\n",
        "results = scraper.search_submissions(subreddit='wholesomememes', \n",
        "                                     filter=['url', 'gildings', 'num_comments', 'num_crossposts'],\n",
        "                                     sort_type='score', limit=10000)\n",
        "\n",
        "# the results object is a generator, so we need to pull the actual data values from the generator object\n",
        "# for more on generators: https://wiki.python.org/moin/Generators\n",
        "memes = pd.DataFrame([item.d_ for item in results])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8GndKnu-_r-"
      },
      "source": [
        "# filtering only image memes, then fetching the images\n",
        "memes = memes[memes['url'].str.contains('https://i.redd.it/')]\n",
        "\n",
        "def get_image(url):\n",
        "  '''\n",
        "  get the image from the specified url\n",
        "  referenced from: https://datascience.stackexchange.com/questions/58351/how-to-retrieve-images-from-a-url-in-a-pandas-dataframe-and-store-them-as-pil-ob\n",
        "  @params url -- the url of the image to get\n",
        "  @return a numpy array of the image if available, else None\n",
        "  '''\n",
        "  request_result = requests.get(url)\n",
        "  try: \n",
        "    request_result.raise_for_status()\n",
        "    image = Image.open(BytesIO(request_result.content))\n",
        "  except requests.exceptions.HTTPError:\n",
        "    return None\n",
        "\n",
        "  return image\n",
        "\n",
        "# the apply function is a dataframe function that lets us apply a function to each row of the dataframe\n",
        "# this is much more efficient than iterating through the dataframe row by row and applying the function\n",
        "#   due to the internal architecture of the function\n",
        "memes['image'] = memes['url'].apply(get_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnszYPmxESFC"
      },
      "source": [
        "# we'll be using the gildings as labels for the data\n",
        "# gildings are reddit awards that cost currency, so we know it's a good metric of the quality\n",
        "# for anything that does have gilding information, convert the dictionary to the total number of awards; we'll use that as the training data\n",
        "# for anything that doesn't have gilding information, we'll use that as the testing data\n",
        "\n",
        "def get_num_gildings(gildings):\n",
        "  # if the gildings is NaN, then it will be a float type\n",
        "  if type(gildings) == float:\n",
        "    return -1\n",
        "\n",
        "  # this is a pythonic way of writing the following code:\n",
        "  # result = 0\n",
        "  # for k,v in gildings.items():\n",
        "  #   result += v\n",
        "  # return result\n",
        "  # where gildings.items() returns key value pairs for each pair in the dictionary\n",
        "\n",
        "  return sum([v for k,v in gildings.items()])\n",
        "\n",
        "# once again, we use the apply method to make this code run per row but faster\n",
        "memes['label'] = memes['gildings'].apply(get_num_gildings)\n",
        "\n",
        "# split into the training and testing datasets\n",
        "# we filter the dataframe by when the label is >= 0 (training, since we have labels)\n",
        "# and when the label is == -1 (testing, since we don't have labels and our function returned -1 when we had no labels)\n",
        "train = memes[memes['label'] >= 0]\n",
        "test = memes[memes['label'] == -1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMaU2q_cDZIO"
      },
      "source": [
        "### Let's learn about the data!\n",
        "\n",
        "We'll be using sklearn's KNeighborsClassifier to perform KNN classification.\n",
        "\n",
        "If a cell or a line of code has `TODO` in it, make sure to substitute it with the correct code! We provided some links to documentation to see what you should fill in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WXkGrhMDnTK"
      },
      "source": [
        "# importing the KNN classifier from sklearn, one of the most popular machine learning packages in python\n",
        "# for documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt  # lets us plot data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26_rYH4GDtPg"
      },
      "source": [
        "# creating a classifier with k=5\n",
        "# the k you choose can affect your results! feel free to change this and try it out\n",
        "classifier = # TODO: make a classifier with k=5! \n",
        "# check https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html for more info on how\n",
        "# you don't need to set any of the other parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7BRRwjsFwiI"
      },
      "source": [
        "# split the training and testing data into data (features) and labels\n",
        "# we typically call the features X and the labels y\n",
        "# we'll also create another array called images\n",
        "\n",
        "# the features we'll use for training (a numerical representation of the meme's popularity) are the number of comments and crossposts. \n",
        "#   we extract the values from the dataframe then convert it to a numpy array for the classifier\n",
        "# the labels we will use are just the labels column, and once again we must convert it to a numpy array\n",
        "X_train = train[['num_comments', 'num_crossposts']].to_numpy(na_value=0)\n",
        "y_train = train['label'].to_numpy()\n",
        "\n",
        "X_test = test[['num_comments', 'num_crossposts']].to_numpy(na_value=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpsWq6mPAN0I"
      },
      "source": [
        "# let's take a look at what the data looks like in space\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train)\n",
        "plt.colorbar()\n",
        "\n",
        "plt.title('r/wholesomememes popularity space')\n",
        "plt.xlabel('number of comments')\n",
        "plt.ylabel('number of crossposts')\n",
        "\n",
        "# when you take a look at the plot, pick any blank space on the graph and figure out what label a KNN would give that point for k=1,3,5\n",
        "# also note that most posts get 0 crossposts -- this is pretty common on reddit\n",
        "# number of comments will probably be the most informative metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW2oHco0GkPi"
      },
      "source": [
        "# fit the training data (that is, traing the KNN classifier on the training data)\n",
        "# TODO: write code to fit the training data! https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.fit\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8PZXfEYHJLz"
      },
      "source": [
        "# predicting the labels (number of gildings) for the test data\n",
        "# TODO: write code to predict on the test data! https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.predict\n",
        "y_pred = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqUMuv5fHK7y"
      },
      "source": [
        "# we don't have real labels for the images that didn't get gilded because they didn't actually get gilded on reddit\n",
        "# but if we take a look at the meme itself, we can evaluate for ourselves whether we think the meme is good\n",
        "\n",
        "def show_image(index, source='test'):\n",
        "  '''\n",
        "  outputs the source image given the index of the image from the array\n",
        "  @param index: index within the array of the sample to show\n",
        "  @param source: whether the image is a test or train image. defaults to test\n",
        "  @return a PIL image object of the meme\n",
        "  '''\n",
        "  # depending on where the image comes from, we index a different array\n",
        "  if source == 'test':\n",
        "    values = X_test[index]\n",
        "    \n",
        "    # filtering the array based on the values of the features\n",
        "    data = test[test['num_comments'] == values[0]]\n",
        "    data = data[data['num_crossposts'] == values[1]]\n",
        "\n",
        "  else:\n",
        "    values = X_train[index]\n",
        "    data = train[train['num_comments'] == values[0]]\n",
        "    data = data[data['num_crossposts'] == values[1]]\n",
        "\n",
        "  try:\n",
        "    return data['image'].iloc[0]\n",
        "  except IndexError:\n",
        "    return 'Couldn\\'t find this image, try another index'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDHmOvdDIsx4"
      },
      "source": [
        "# run this cell and the next cell to see the results for a meme of your choice\n",
        "index = 3 # TODO: change this to see different images and scores!\n",
        "\n",
        "show_image(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ_DOT_QJHXm"
      },
      "source": [
        "# find out the rating of the meme you just showed!\n",
        "print('The classifier thought that this meme would have gotten ' + str(y_pred[index]) + ' gildings')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}